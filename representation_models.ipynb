{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de representación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias necesarias\n",
    "import os\n",
    "\n",
    "# Matplotlib conf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "params = {'legend.fontsize': 'x-large',\n",
    "          'figure.figsize': (15, 5),\n",
    "         'axes.labelsize': 'x-large',\n",
    "         'axes.titlesize':'x-large',\n",
    "         'xtick.labelsize':40,\n",
    "         'ytick.labelsize': 40\n",
    "}\n",
    "plt.rcParams.update(params)\n",
    "# Seaborn conf\n",
    "import seaborn as sns\n",
    "sns.set(style='darkgrid')\n",
    "sns.set_palette(sns.color_palette(\"Blues\"))\n",
    "\n",
    "import sys\n",
    "\n",
    "#Procesado de datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "#Modelos\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from nltk import ngrams\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carga de los conjuntos de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los conjuntos de datos que se utilizarán son los que se persistieron tras el preproceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_training = pd.read_csv(\"./data/ds_training_processed.csv\")\n",
    "df_testing = pd.read_csv(\"./data/ds_testing_processed.csv\")\n",
    "\n",
    "X_train = df_training[\"text\"]\n",
    "y_train = df_training[\"sentiment\"]\n",
    "X_test = df_testing[\"text\"]\n",
    "y_test = df_testing[\"sentiment\"]\n",
    "\n",
    "y_train_2 = to_categorical(y_train, num_classes=2)\n",
    "y_test_2 = to_categorical(y_test, num_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conexión con el preprocesado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dado que se pretende crear diversos modelos para analizarlos y luego realizar comparativas, será necesario utilizar diferentes modelos de representación. A pesar de esto, no resultaría lógico combinar algunos clasificadores con algunos modelos de representación. Pero, en cada uno de esos casos se justificará.\n",
    "\n",
    "Con el propósito de crear estas representaciones, ya se adelantó en el preproceso la necesidad de reducir el vocabulario mediante las siguientes técnicas:\n",
    "* Se eliminarán todas las palabras que solo aparezcan en 5 documentos.\n",
    "* Se eliminarán todas las palabras que aparezcan en el 90% de los documentos.\n",
    "\n",
    "Ciertamente, en el preproceso se planteó la reducción de vocabulario con el propósito de comprender mejor las tablas de frecuencias máximas. Pero, esta medida **también tiene otro propósito que es el de reducir la dimensionalidad de los modelos de representación**. Y esto se puede demostrar así:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27949,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(8144,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_cv = CountVectorizer(binary=True)\n",
    "base_cv.fit(X_train)\n",
    "\n",
    "vsm_binary_cv = CountVectorizer(binary=True, max_df=0.9, min_df=5)\n",
    "vsm_binary_cv.fit(X_train)\n",
    "\n",
    "display(\n",
    "    base_cv.get_feature_names_out().shape,\n",
    "    vsm_binary_cv.get_feature_names_out().shape\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que la dimensionalidad del vocabulario del modelo de representación que ostenta las medidas adoptadas sobre las frecuencias de documento máximas y mínimas sobre el vocabulario, presenta una reducción de más de 4 veces menor que la base del experimento (sin las medidas).\n",
    "\n",
    "Entonces, se ha demostrado una reducción de dimensionalidad mediante las medidas sobre las frecuencias adoptadas. De modo que en la construcción de los modelos de representación, ya se contará con este beneficio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construcción de los modelos de representación en espacios vectoriales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estos serán los modelos de representación que se crearán en espacios vectoriales:\n",
    "* VSM + pesado binario + redimensionamiento por SVD (LSI).\n",
    "* VSM + pesado binario + bigramas (ngrams) + redimensionamiento por SVD (LSI).\n",
    "* VSM + pesado TF-IDF + redimensionamiento por SVD (LSI).\n",
    "* VSM + pesado TF-IDF + bigramas (ngrams) + redimensionamiento por SVD (LSI)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VSM + pesado binario + redimensionamiento por SVD (LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsm_binary_cv = CountVectorizer(binary=True, max_df=0.9, min_df=5)\n",
    "vsm_binary_cv.fit(X_train)\n",
    "svd = TruncatedSVD(n_components=5000)\n",
    "vsm_binary = {\n",
    "  \"model\": \"VSM + pesado binario + redimensionamiento por SVD (LSI)\",\n",
    "  \"train_text_features\": svd.fit_transform( vsm_binary_cv.transform(X_train) ),\n",
    "  \"test_text_features\": svd.transform( vsm_binary_cv.transform(X_test) )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VSM + pesado binario + bigramas (ngrams) + redimensionamiento por SVD (LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsm_binary_ngrams_cv = CountVectorizer(binary=True, ngram_range=(1,2), max_df=0.9, min_df=5)\n",
    "svd = TruncatedSVD(n_components=5000)\n",
    "vsm_binary_ngrams = {\n",
    "  \"model\": \"VSM + pesado binario + rango de n-grams [1 y 2] + redimensionamiento por SVD (LSI)\",\n",
    "  \"train_text_features\": svd.fit_transform( vsm_binary_ngrams_cv.fit_transform(X_train) ),\n",
    "  \"test_text_features\": svd.transform( vsm_binary_ngrams_cv.transform(X_test) )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VSM + pesado TF-IDF + redimensionamiento por SVD (LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsm_tfidf_v = TfidfVectorizer(norm='l2', use_idf=True, max_df=0.9, min_df=5)\n",
    "svd = TruncatedSVD(n_components=5000)\n",
    "vsm_tfidf = {\n",
    "  \"model\": \"VSM + pesado TF-IDF + redimensionamiento por SVD (LSI)\",\n",
    "  \"train_text_features\": svd.fit_transform( vsm_tfidf_v.fit_transform(X_train) ),\n",
    "  \"test_text_features\": svd.transform( vsm_tfidf_v.transform(X_test) )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VSM + pesado TF-IDF + bigramas (ngrams) + redimensionamiento por SVD (LSI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsm_tfidf_ngrams_v = TfidfVectorizer(norm='l2', use_idf=True, ngram_range=(1,2), max_df=0.9, min_df=5)\n",
    "svd = TruncatedSVD(n_components=5000)\n",
    "vsm_tfidf_ngrams = {\n",
    "  \"model\": \"VSM + pesado TF-IDF + rango de n-grams [1 y 2] + redimensionamiento por SVD (LSI)\",\n",
    "  \"train_text_features\": svd.fit_transform( vsm_tfidf_ngrams_v.fit_transform(X_train) ),\n",
    "  \"test_text_features\": svd.transform( vsm_tfidf_ngrams_v.transform(X_test) )\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación de modelos de representación con word embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los modelos que se crearán son los siguientes:\n",
    "* Word embedding: tokenizando el texto en secuencias y *pad_sequences*, para que tengan todos los textos de las observaciones el mismo tamaño.\n",
    "* Word embedding con embeddings pre-entrenados (usando GloVe)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding: tokenizando el texto en secuencias y *pad_sequences*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Primero se crea los tokens y las secuencias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 29772 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
    "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "print('Found %s unique tokens.' % len(tokenizer.word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se han encontra 29.772 tokens diferentes en todo el conjunto de entrenamiento, lo cual define claramente el tamaño del vocabulario.\n",
    "\n",
    "Por otra parte, ahora se dispone de secuencias de diferentes tamaños. Es decir: Un tamaño específico por cada documento. Por ejemplo, si el documento es de 2 palabras el tamaño de la secuencia será de 2 y si el documento es de 50 palabras, la secuencia será de 50.\n",
    "\n",
    "Con el propósito de solventar el problema mencionado en el párrafo anterior, es necesario crear unificar todos los tamaños de las secuencias generadas. Para ello, se debe considerar cuál es el tamaño óptimo de la cadena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9993"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1297"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "more_than_600: 26\n",
      "more_than_700: 20\n",
      "more_than_900: 8\n",
      "more_than_1000: 6\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "more_than_600 = 0\n",
    "more_than_700 = 0\n",
    "more_than_900 = 0\n",
    "more_than_1000 = 0\n",
    "for item in sequences_train:\n",
    "  if len(item) > max_len:\n",
    "    max_len = len(item)\n",
    "  if len(item) > 600:\n",
    "    more_than_600 = more_than_600 + 1\n",
    "  if len(item) > 700:\n",
    "    more_than_700 = more_than_700 + 1\n",
    "  if len(item) > 900:\n",
    "    more_than_900 = more_than_900 + 1\n",
    "  if len(item) > 1000:\n",
    "    more_than_1000 = more_than_1000 + 1\n",
    "\n",
    "display(\n",
    "    len(sequences_train),\n",
    "    max_len\n",
    ")\n",
    "print(\"more_than_600:\",more_than_600)\n",
    "print(\"more_than_700:\",more_than_700)\n",
    "print(\"more_than_900:\",more_than_900)\n",
    "print(\"more_than_1000:\",more_than_1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que:\n",
    "* La longitud máxima de secuencias es de 9.993. Es decir, que el mayor número de tokens que se encuentra en un texto es de 9.993.\n",
    "* Por lo tanto, se podría estimar que el documento con mayor cantidad de tokens defina el tamaño de todas las secuencias. Aunque, **esta solución no sería adecuada porque produce más espacidad y dimensionalidad**.\n",
    "* Por eso se ha contado cuántos documentos tienen un determinado número de tokens, contando a partir de 600 hasta 1000, considerando que 9.993 es un número que podría despreciarse. Sobre estos cortes, se puede observar que el número de documentos de más de 600 tokens es de 26. Y que este número se reduce poco a poco conforme se va aumentando el número de tokens.\n",
    "* En conclusión, el número de documentos que tiene una longitud de 600 es apenas de 26, un número de documentos despreciable que supone apenas un *0.26%* del total de 9.993 documentos que se disponen.\n",
    "\n",
    "Entonces, como ya se ha podido definir un número de secuencias para cortar y hacer padding en los casos que sea necesarios, se procede a hacer la operación y así crear finalmente el modelo de representación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 600\n",
    "embedding_padding = {\n",
    "  \"model\": \"Word embedding: tokenizando el texto en secuencias y pad_sequences\",\n",
    "  \"train_text_features\": pad_sequences(sequences_train, maxlen=max_len),\n",
    "  \"test_text_features\": pad_sequences(sequences_test, maxlen=max_len)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embedding con embeddings pre-entrenados (usando GloVe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para la creación de este modelo de representación:\n",
    "* Se usará Glove entrenado con Wikipedia 2014 para generar representaciones de palabras en 50 dimensiones. Esto se descarga de:\n",
    "    * https://nlp.stanford.edu/projects/glove/\n",
    "    * Específicamente, se descarga el siguiente fichero: https://nlp.stanford.edu/data/glove.6B.zip\n",
    "    * Se utilizará exactamente el fichero glove.6B.50d.txt\n",
    "* Se fijará un tamaño de vocabulario de 50.000 palabras. Considerando que esta dimensionalidad tendrá un impacto en los tiempos de entrenamiento de los modelos clasificadores que se vayan a crear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensión de la capa de embeddings\n",
    "emb_dim = 50\n",
    "#Número de palabras tenidas en cuenta (reducción del vocabulario)\n",
    "n_most_common_words = 50000\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('./glove/glove.6B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "num_words = min(n_most_common_words, len(tokenizer.word_index)) + 1\n",
    "embedding_matrix = np.zeros((num_words, emb_dim))\n",
    "\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i > n_most_common_words:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # Si la palabra existe añadimos su vector a la matriz\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    else:\n",
    "        # Si no existe, se le asigna un vector aleatorio\n",
    "        embedding_matrix[i] = np.random.randn(emb_dim)\n",
    "\n",
    "embedding_glove = {\n",
    "  \"model\": \"Word embedding with GloVe\",\n",
    "  \"train_text_features\": pad_sequences(sequences_train, maxlen=max_len),\n",
    "  \"test_text_features\": pad_sequences(sequences_test, maxlen=max_len),\n",
    "  \"embedding_matrix\": embedding_matrix\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Almacenamiento de los modelos de representación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hay que considerar que el proceso de generar los modelos de representación se ve afectados por la reducción de dimensionalidad aplicada y que este proceso está tomando alrededor de 40 minutos. Así que, como los modelos de representación se utilizarán en diferentes fases, se deben de poder almacenar a los efectos de evitar repetir su proceso de creación, tal y como se haría con los modelos clasificadores.\n",
    "\n",
    "Para ello se utilizará un sistema de persistencia basado en la persistencia de datos de *numpy*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_representation_model(model_to_save, file_name):\n",
    "    with open(\"./repmodels/\"+file_name+\".txt\", 'w+') as f:\n",
    "        f.write(model_to_save[\"model\"])\n",
    "    \n",
    "    np.save(\"./repmodels/\"+file_name+\"_train.npy\", model_to_save[\"train_text_features\"])\n",
    "    np.save(\"./repmodels/\"+file_name+\"_test.npy\", model_to_save[\"test_text_features\"])\n",
    "\n",
    "    if \"embedding_matrix\" in model_to_save:\n",
    "        np.save(\"./repmodels/\"+file_name+\"_embeddingmatrix.npy\", model_to_save[\"embedding_matrix\"])\n",
    "\n",
    "def load_representation_model(file_name, include_embedding_matrix=False):\n",
    "    res = {}\n",
    "    with open(\"./repmodels/\"+file_name+\".txt\", 'r') as d:\n",
    "        res[\"model\"] = d.readline()\n",
    "    res[\"train_text_features\"] = np.load(\"./repmodels/\"+file_name+\"_train.npy\")\n",
    "    res[\"test_text_features\"] = np.load(\"./repmodels/\"+file_name+\"_test.npy\")\n",
    "\n",
    "    if include_embedding_matrix:\n",
    "        res[\"embedding_matrix\"] = np.load(\"./repmodels/\"+file_name+\"_embeddingmatrix.npy\")\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_representation_model(vsm_binary, \"vsm_binary\")\n",
    "save_representation_model(vsm_binary_ngrams, \"vsm_binary_ngrams\")\n",
    "save_representation_model(vsm_tfidf, \"vsm_tfidf\")\n",
    "save_representation_model(vsm_tfidf_ngrams, \"vsm_tfidf_ngrams\")\n",
    "save_representation_model(embedding_padding, \"embedding_padding\")\n",
    "save_representation_model(embedding_glove, \"embedding_glove\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
